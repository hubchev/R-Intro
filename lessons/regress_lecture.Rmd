---
title: "Regression Analysis"
author: "Stephan.Huber@hs-fresenius.de"
date: "December 2020 -- HS-Fresenius: Data Science Course"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Preface

## Literature

Regression analysis is covered by almost all econometric and statistical textbooks. Some are more formal some are more illustrative and intuitive. Here is my selection with a focus on R:

- [Book: Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/)
- [Book: Introduction to Econometrics with R](https://www.econometrics-with-r.org/)
- [A more intuitive approach by Jim](https://statisticsbyjim.com)
- A more formal approach from my alma mater: [slides](https://www.uni-regensburg.de/wirtschaftswissenschaften/vwl-tschernig/medien/zeitreihenoekonometrie/eoe_eng_2020_08_chapter_all.pdf) [handout](https://www.uni-regensburg.de/wirtschaftswissenschaften/vwl-tschernig/medien/methoden-der-oekonometrie/methoden_oekonometrie_handout_2018_11_29.pdf)

## Learning Objectives

- 

## Why to care about regression analysis

![](~/Dropbox/hsf/pic/destat/afact2.gif)

- Regressions allow us to **draw insights** from data, 
- to analyze and **interpret** the strength of relationships and 
- to reduce the likeliness of **causal fallacy**.

---

- **Linear regression** is a predictive modeling techniques that aims to find a mathematical equation for a variable $y$ as a function of one (simple linear model) or more variables (multiple linear regression), $x$. 

- The method to \textit{fit a line} is called the **ordinary least squared (OLS) method** as it minimizes the sum of the squared differences of all $y_i$ and $y_i^*$ as sketched below.

![](~/Dropbox/hsf/pic/destat/regression_ols.png){ width=50% }


---

The simple linear regression model is
\[
y_i = \beta_{0} + \beta_{1} x_i + \epsilon_i
\]
where

-  the index $i$ runs over the observations, $i=1,\dots,n$
-  $y_i$ is the \textbf{dependent variable}, the regressand
-  $x_i$ is the \textbf{independent variable}, the regressor
-  $\beta_0$ is the \textbf{intercept} or constant
-  $\beta_1$ is the slope of regression line
-  $\epsilon_i$ is the \textbf{error term} or the residual.

## OLS estimation method

- minimize the squared residuals by choosing the estimated coefficients $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$ 

\[\min_{\hat{\beta_{0}}, \hat{\beta_{1}}}\sum_{i=1} \epsilon_i^2  =  \sum_{i=1}	(y_i - \hat{\beta_{0}} - \hat{\beta_{1}} x_i)^2\]

-  Minimizing the function requires to calculate the first order conditions with respect to $\hat{\beta_{0}}$ and $\hat{\beta_{1}}$  and set them zero (see exercises)

- The estimators are:
\[\hat{\beta_{0}} =\bar{y}-\hat{\beta_{1}}\bar{x}\]

\[\hat{\beta_{1}} =\frac{\sum_{i=1}(y_i -\bar{y})(x_i-\bar{x})}{\sum_{i=1} (\bar{x} -  x_i)^2 }\]

---

In the multiple regression model the OLS is derived similarly but we skip the derivation. The $\hat{\beta}$ coeficient vector can be expressed as follows:
\[\hat{\beta}=\left(\mathrm{X} ^{\mathsf {T}}\mathrm {X} \right)^{-1}\mathrm {X} ^{\mathsf {T}}\mathbf {y}\]
![](~/Dropbox/hsf/pic/destat/intuitive_1.png){ width=30% } ![](~/Dropbox/hsf/pic/destat/intuitive_2.png){ width=40% }

## Caveats of OLS

On this [website](https://www.econometrics-with-r.org/4-2-estimating-the-coefficients-of-the-linear-regression-model.html) you find an interactive application. Play around with it and discuss possible caveats of the OLS method.

![](~/Dropbox/hsf/pic/destat/reg-outlier.png){ width=85% }

## Example 

In the statistic course of WS 2020, I asked 23 students about their weight, height, sex, and number of siblings:

```{r, echo = TRUE}
library("haven")
classdata <- read.csv("~/Dropbox/hsf/courses_202/destat/classdata/classdata.csv")

head(classdata)
```

---

```{r, echo = TRUE}

summary(classdata)
```


## How to execute a regression analysis

1. get known to the data 
2. build a theory on how the variables may be related
3. derive a estimated equation from your theory
4. estimate
5. evaluate your empirics
6. go back to 2. and improve your theory
7. interpret your results

A more elaborated flow diagram for regression analysis can be found below. <sub><sup><sub>
Source: <http://medrescon.tripod.com/regression_explained.pdf>
</sup></sub></sup>

---

![](~/Dropbox/hsf/pic/destat/olsscheme.png){ width=70% }

## First look at data


```{r pressure, echo=TRUE}
library("ggplot2")
ggplot(classdata, aes(x=height, y=weight)) + geom_point() 

```

---

include a regression line:

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight)) +
  geom_point() +
  stat_smooth(formula=y~x, method="lm", se=FALSE, colour="red", linetype=1)

```

---

distinguish male/female by including a seperate constant:

```{r , echo=TRUE}
## baseline regression  model
model  <- lm(weight ~ height + sex , data = classdata )
show(model)
interm <- model$coefficients[1] 
slope  <- model$coefficients[2]
interw <- model$coefficients[1]+model$coefficients[3] 
```

---

```{r, echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point() +
  geom_abline(slope = slope, intercept = interw, linetype = 2, size=1.5)+
  geom_abline(slope = slope, intercept = interm, linetype = 2, size=1.5) +
  geom_abline(slope = coef(model)[[2]], intercept = coef(model)[[1]]) 

```

does not look to good, maybe we should introduce also different slopes for m/w

---

```{r , echo=TRUE}

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  method = "lm", 
              se = FALSE, colour = "red", linetype = 1)

```


---

Can we use other available variables: siblings?

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = siblings)) 

```

---

```{r , echo=FALSE}
## baseline model
model  <- lm(weight ~ height + sex , data = classdata )
interw <- model$coefficients[1]+model$coefficients[3] 
interm <- model$coefficients[1] 
slope  <- model$coefficients[2]

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  
              method = "lm", 
              se = FALSE, 
              colour = "red", 
              linetype = 1)

```


---

Let us look at regression output:

```{r, echo=TRUE, results='hide'}

m1 <- lm(weight ~ height , data = classdata )
m2 <- lm(weight ~ height + sex , data = classdata )
m3 <- lm(weight ~ height + sex + height * sex , data = classdata )
m4 <- lm(weight ~ height + sex + height * sex + siblings , data = classdata )
m5 <- lm(weight ~ height + sex + height * sex , data = subset(classdata, siblings < 4 ))

library(sjPlot)
tab_model(m1, m2, m3, m4, m5,
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

---

```{r, echo=FALSE}
tab_model(m1, m2, m3, m4, 
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

---

excluding outliers with four siblings:

```{r, echo=FALSE}
tab_model(m3, m5,
          p.style = "stars",
          p.threshold = c(0.2, 0.1, 0.05),
          show.ci = FALSE, 
          show.se = FALSE) 
```

---

## Interpretation of the results

- We can make predictions about the impact of height on male and female
- As both, the intercept and the slope differs for male and female we should interpret the regressions seperately:
- One centimeter more for **MEN** is *on average* and *ceteris paribus* related with 0.16 kg more weight.
- One centimeter more for **WOMEN** is *on average* and *ceteris paribus* related with 1.01 kg more weight.

## Regression Diagnostics

Linear Regression makes several assumptions about the data, the model assumes that:

- The relationship between the predictor (x) and the dependent variable (y) has linear relationship.
- The residuals are assumed to have a constant variance.
- The residual errors are assumed to be normally distributed.
- Error terms are independent and have zero mean.

More on regression Diagnostics can be found [Applied Statistics with R: 13 Model Diagnostics](https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html#r-markdown-6)

---

```{r, echo=T}
plot(residuals(m3), fitted(m3))
plot(residuals(m3), classdata$siblings)
```

## R squared -- a measures of fit

![](~/Dropbox/hsf/pic/destat/howbad.png){ width=80% }

---

$R^2$ is the fraction of the sample variance of $Y_i$ that is explained by $X_i$.  It can be written as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS):
\[ESS  =  \sum_{i = 1}^n \left( \hat{Y_i} - \bar{Y} \right)^2,\]
\[TSS  =  \sum_{i = 1}^n \left( Y_i - \bar{Y} \right)^2,\]
\[R^2  = \frac{ESS}{TSS}.\]

---

Since $TSS = ESS + SSR$ we can also write 

$R^2 = 1- \frac{SSR}{TSS}$ 

with $SSR=\sum_{i=1} \epsilon_i^2$

![](~/Dropbox/hsf/pic/destat/fitR.png){ width=55% }


$R^2$ lies between 0 and 1. It is easy to see that a perfect fit, i.e., no errors made when fitting the regression line, implies $R2=1$ since then we have $SSR=0$. On the contrary, if our estimated regression line does not explain any variation in the $Y_i$, we have $ESS=0$ and consequently $R^2=0$.

## Adjusted R-squared

Including more independent variables into an estimated model must decrease SSR. Thus, the R-squared never decreases, when adding new variables even when it just a chance correlation between variables. Having more coefficients to estimate the precision at which we can estimate the effects decrease. Thus, we need to deal with the trade-off. The adjusted $R^2$ can help here:  

\[{\displaystyle {\bar {R}}^{2}=1-(1-R^{2}){n-1 \over n-p-1}}\]

Always use adjusted $R^2$ when you compare specifications with different number of ceofficients.
  

## Simpsons paradox

![](~/Dropbox/hsf/pic/destat/foo-13.png)

---

![](~/Dropbox/hsf/pic/destat/foo-32.png)


## Correlation does not imply causation

![](~/Dropbox/hsf/pic/destat/storks.png){ width=75% }

---	

If a strong correlation is observed between two variables A and B, there are several possible explanations: 

 (a) A influences B; 
 (b) B influences A; 
 (c) A and B are influenced by one or more additional variables; 
 (d) the relationship observed between A and B was a chance error.
 
 ![](~/Dropbox/hsf/pic/destat/storks3.png){ width=25% }

## The miracle of CONTROL VARIABLES in multiple regressions

Control variables are usually variables that you are not particularly interested in, but that are related to the dependent variable. You want to remove their effects from the equation. A control variable enters a regression in the same way as an independent variable -- the method is the same.

 ![](~/Dropbox/hsf/pic/destat/storks3.png){ width=25% }
 

## When we need (more) control variables

From the [Gauss-Markov theorem](https://www.econometrics-with-r.org/5-5-the-gauss-markov-theorem.html) we know that if the [OLS assumptions](https://www.econometrics-with-r.org/6-4-ols-assumptions-in-multiple-regression.html) are fullfiled, the OLS estimator is (in the sense of smallest variance) the **best linear conditionally unbiased estimator (BLUE)**.

However, OLS estimates can suffer from **omitted variable bias** when the regressor, X, is correlated with an omitted variable. For omitted variable bias to occur, two conditions must be fulfilled:

    1. X is correlated with the omitted variable.
    2. The omitted variable is a determinant of the dependent variable Y.

---

![](~/Dropbox/hsf/pic/destat/storks2.png){ width=80% }



## Take away messages

- Regressions rule the world and may kill alternative facts
- correlation does not imply causation
- it is hard to find a the true *data generating process*


![](~/Dropbox/hsf/pic/destat/correlation_causation.png){ width=80% }


## Exercises

Read ch. 10 of the book [Econometrics with R](https://www.econometrics-with-r.org/10-rwpd.html) and analyze with a multiple regression analyis if a beertax is negative associated with traffic deaths.
Use the *Fatalities* data from the AER package.


## Test questions

- Is the mean of the dependent and the independent variable on the regression line?

True or false:

- If the OLS assumptions are fullfiled then the residuals are expected to be normally distributed with a mean of zero.
- If a potential explanatory (independent) variable is corelated with one or more variables that are part of the estimated model, we suffer omitted variable bias. 
- The problem of the unobservable heterogeneity is a major problem in empirical research. Using OLS to explain the dependent variable, $Y$,  with an independent variable, $X_1$ may yield a bias in the estimation of the impact of the independent variable, $X_2$, if and only if $X_1$ and $X_2$ are corelated.













